{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a656adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!  mkdir -p ~/aiffel/text_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae5c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is Joseph . \n"
     ]
    }
   ],
   "source": [
    "# 문장부호 : 분리해서 부호가 앞 단어와 별도임을 알려줘야함\n",
    "\n",
    "def pad_punctuation(sentence, punc) :\n",
    "    for p in punc :\n",
    "        sentence = sentence.replace(p,\" \" + p +\" \")\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is Joseph.\"\n",
    "\n",
    "print(pad_punctuation(sentence,[\".\", \"?\", \"!\", \",\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa81818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n"
     ]
    }
   ],
   "source": [
    "# lower : computer understands lower and capital letter differently\n",
    "\n",
    "sentence = \"First, open the first chapter.\"\n",
    "print(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788d95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy\n"
     ]
    }
   ],
   "source": [
    "# 특수문자\n",
    "\n",
    "import re\n",
    "\n",
    "sentence = 'He is a ten-year-old boy'\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\",\" \", sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6818d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n",
    "Some one was drawing water and my teacher placed my hand under the spout. \n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n",
    "I stood still, my whole attention fixed upon the motions of her fingers. \n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n",
    "That living word awakened my soul, gave it light, hope, joy, set it free! \n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32918ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text, punc, regex) :\n",
    "    for p in punc :\n",
    "        text = text.replace(p,\" \" + p + \" \")\n",
    "        \n",
    "    text = re.sub(regex,\" \",text).lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a90b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
      "but my teacher had been with me several weeks before i understood that everything has a name . \n",
      "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
      "some one was drawing water and my teacher placed my hand under the spout .  \n",
      "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
      "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
      "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
      "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
      "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
      "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cleaning_text(corpus, ['.','?','!',','], '([^a-zA-Z0-9,.,?!\\n])'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6051dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'days', 'that', 'followed', 'I', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words,', 'among', 'them', 'pin,', 'hat,', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit,', 'stand', 'and', 'walk.', 'But', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'I', 'understood', 'that', 'everything', 'has', 'a', 'name.', 'One', 'day,', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well-house,', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered.', 'Some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout.', 'As', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water,', 'first', 'slowly,', 'then', 'rapidly.', 'I', 'stood', 'still,', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers.', 'Suddenly', 'I', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten—a', 'thrill', 'of', 'returning', 'thought;', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me.', 'I', 'knew', 'then', 'that', '\"w-a-t-e-r\"', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand.', 'That', 'living', 'word', 'awakened', 'my', 'soul,', 'gave', 'it', 'light,', 'hope,', 'joy,', 'set', 'it', 'free!', 'There', 'were', 'barriers', 'still,', 'it', 'is', 'true,', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away.']\n"
     ]
    }
   ],
   "source": [
    "# 공백 기반 토큰화\n",
    "\n",
    "tokens = corpus.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d05f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Komoran, Mecab, Okt\n",
    "# from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a9e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_list = [Hannanum(), Kkma(), Komoran(), Mecab(), Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36183d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hannanum] \n",
      "[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n",
      "[Kkma] \n",
      "[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
      "[Komoran] \n",
      "[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "[Mecab] \n",
      "[('코로나', 'NNP'), ('바이러스', 'NNG'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNBC'), ('12', 'SN'), ('월', 'NNBC'), ('중국', 'NNP'), ('우한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('한', 'XSV+ETM'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKB'), ('확산', 'NNG'), ('된', 'XSV+ETM'), (',', 'SC'), ('새로운', 'VA+ETM'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n",
      "[Okt] \n",
      "[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "for tokenizer in tokenizer_list :\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0be93b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('l', 'o'): 7,\n",
       "             ('o', 'w'): 7,\n",
       "             ('w', 'e'): 8,\n",
       "             ('e', 'r'): 2,\n",
       "             ('n', 'e'): 6,\n",
       "             ('e', 'w'): 6,\n",
       "             ('e', 's'): 9,\n",
       "             ('s', 't'): 9,\n",
       "             ('w', 'i'): 3,\n",
       "             ('i', 'd'): 3,\n",
       "             ('d', 'e'): 3})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {\n",
    "    'l o w' :5,\n",
    "    'l o w e r' :2,\n",
    "    'n e w e s t' : 6,\n",
    "    'w i d e s t' :3\n",
    "}\n",
    "\n",
    "pairs = get_stats(vocab)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79332b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Life is too short, You need python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a91ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = max(pairs, key = pairs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73dbf596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e\\\\ s'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_out = {}\n",
    "bigram = re.escape(' '.join(best))\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0387bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n e w es t'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "p.sub(''.join(best), 'n e w e s t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d637749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l o w\n",
      "l o w e r\n",
      "n e w e s t\n",
      "w i d e s t\n"
     ]
    }
   ],
   "source": [
    "for word in vocab :\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in :\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0]+pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "for word in v_in :\n",
    "    w_out = p.sub(''.join(pair), word)\n",
    "    v_out[w_out] = v_in[word]\n",
    "\n",
    "return v_out, pair[0]+pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_stats(vocab)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    \n",
    "    print('next subword pair: ', merge_tok)\n",
    "    print('changed subword pair:', vocab, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb23c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>step 1\n",
      "next subword pair:  es\n",
      "changed subword pair: {'l o w': 5, 'l o w e r': 2, 'n e w es t': 6, 'w i d es t': 3} \n",
      "\n",
      ">>step 2\n",
      "next subword pair:  est\n",
      "changed subword pair: {'l o w': 5, 'l o w e r': 2, 'n e w est': 6, 'w i d est': 3} \n",
      "\n",
      ">>step 3\n",
      "next subword pair:  lo\n",
      "changed subword pair: {'lo w': 5, 'lo w e r': 2, 'n e w est': 6, 'w i d est': 3} \n",
      "\n",
      ">>step 4\n",
      "next subword pair:  low\n",
      "changed subword pair: {'low': 5, 'low e r': 2, 'n e w est': 6, 'w i d est': 3} \n",
      "\n",
      ">>step 5\n",
      "next subword pair:  ne\n",
      "changed subword pair: {'low': 5, 'low e r': 2, 'ne w est': 6, 'w i d est': 3} \n",
      "\n",
      "merged vocab: ['es', 'est', 'lo', 'low', 'ne']\n"
     ]
    }
   ],
   "source": [
    "# oov 해결하는 방법, Wordpiece model(BPE)\n",
    "\n",
    "#example\n",
    "\n",
    "import re, collections\n",
    "\n",
    "vocab = {\n",
    "    'l o w' :5,\n",
    "    'l o w e r' :2,\n",
    "    'n e w e s t' : 6,\n",
    "    'w i d e s t' :3\n",
    "}\n",
    "\n",
    "num_merges =5 \n",
    "\n",
    "def get_stats(vocab) :\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        \n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "            \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in :\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0]+pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print('>>step {0}'.format(i+1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    \n",
    "    print('next subword pair: ', merge_tok)\n",
    "    print('changed subword pair:', vocab, '\\n')\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print('merged vocab:', token_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f6146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
