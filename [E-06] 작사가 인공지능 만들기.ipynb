{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c23414",
   "metadata": {},
   "source": [
    "# ğŸ“ LSTM í”„ë¡œì íŠ¸: ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1874188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import glob  #glob ëª¨ë“ˆì˜ glob í•¨ìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì œì‹œí•œ ì¡°ê±´ì— ë§ëŠ” íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•œë‹¤\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6310d3b",
   "metadata": {},
   "source": [
    "# Step 1. ë°ì´í„° ì¤€ë¹„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMS ë‹¤ìš´ë¡œë“œ\n",
    "# lyrics =os.listdir(os.getenv('HOME') + '/aiffel/lyricist/data/lyrics')\n",
    "\n",
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(\"lyrics.zip\", mode=\"a\") as archive:\n",
    "#     for i in lyrics:\n",
    "#         archive.write(os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/'+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa6da9",
   "metadata": {},
   "source": [
    "# Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6985d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['', '', 'All of this and more is for you']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = '/Users/seungyonglee/Desktop/Life3/aiffel/project/data/lyrics'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path + '/*.txt') #txt_file_path ê²½ë¡œì— ìˆëŠ” ëª¨ë“  íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ txt_list ì— í• ë‹¹\n",
    "raw_corpus = [] \n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : íŒŒì¼ ì „ì²´ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì½ì–´ì˜¨ë‹¤. , splitlines()  : ì—¬ëŸ¬ë¼ì¸ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆëŠ” ë¬¸ìì—´ì„ í•œë¼ì¸ì”© ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "        raw_corpus.extend(raw) # extend() : ë¦¬ìŠ¤íŠ¸í•¨ìˆ˜ë¡œ ì¶”ê°€ì ì¸ ë‚´ìš©ì„ ì—°ì¥ í•œë‹¤.\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d8fae",
   "metadata": {},
   "source": [
    "# Step 3. ë°ì´í„° ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9041d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e25e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14905565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> all of this and more is for you <end>',\n",
       " '<start> with love , sincerity and deepest care <end>',\n",
       " '<start> my life with you i share <end>',\n",
       " '<start> ever since i met you , baby <end>',\n",
       " '<start> i ve been wantin to lay you down <end>',\n",
       " '<start> but it s so hard to get you <end>',\n",
       " '<start> baby , when you never come around <end>',\n",
       " '<start> every day that you keep it away <end>',\n",
       " '<start> it only makes me want it more <end>',\n",
       " '<start> ooh baby , just say the word <end>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "# raw_corpus listì— ì €ì¥ëœ ë¬¸ì¥ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë°˜í™˜í•˜ì—¬ sentenceì— ì €ì¥\n",
    "for sentence in raw_corpus:\n",
    "    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # ì•ì„œ êµ¬í˜„í•œ preprocess_sentence() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c382236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f4e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  24  19 ...   0   0   0]\n",
      " [  2  31  33 ...   0   0   0]\n",
      " [  2  13 102 ...   0   0   0]\n",
      " ...\n",
      " [  2  39  39 ...   0   0   0]\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2  39  39 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x7fb51bdb80a0>\n"
     ]
    }
   ],
   "source": [
    "# í† í°í™” í•  ë•Œ í…ì„œí”Œë¡œìš°ì˜ Tokenizerì™€ pad_sequencesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "# ë” ì˜ ì•Œê¸° ìœ„í•´ ì•„ë˜ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizerë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
    "    # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”\n",
    "    # 7000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤\n",
    "    # tokenizer.fit_on_texts(texts): ë¬¸ì ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    # tokenizer.texts_to_sequences(texts): í…ìŠ¤íŠ¸ ì•ˆì˜ ë‹¨ì–´ë“¤ì„ ìˆ«ìì˜ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b10a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34cc733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 24 19 42  8 98 26 28  7  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "[24 19 42  8 98 26 28  7  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dbb3508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(256, 346), dtype=tf.int32, name=None), TensorSpec(shape=(256, 346), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ\n",
    " # tokenizer.num_words: ì£¼ì–´ì§„ ë°ì´í„°ì˜ ë¬¸ì¥ë“¤ì—ì„œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ nê°œì˜ ë‹¨ì–´ë§Œ ì„ íƒ\n",
    " # tokenize() í•¨ìˆ˜ì—ì„œ num_wordsë¥¼ 7000ê°œë¡œ ì„ ì–¸í–ˆê¸° ë•Œë¬¸ì—, tokenizer.num_wordsì˜ ê°’ì€ 7000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# ì¤€ë¹„í•œ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "# ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\n",
    "# ìì„¸íˆ ì•Œì•„ë‘˜ìˆ˜ë¡ ë„ì›€ì´ ë§ì´ ë˜ëŠ” ì¤‘ìš”í•œ ë¬¸ì„œì…ë‹ˆë‹¤\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bbc5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding ë ˆì´ì–´, 2ê°œì˜ LSTM ë ˆì´ì–´, 1ê°œì˜ Dense ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\n",
    "        # Embedding ë ˆì´ì–´ëŠ” ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ ê°’ì„ í•´ë‹¹ ì¸ë±ìŠ¤ ë²ˆì§¸ì˜ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿”ì¤€ë‹¤.\n",
    "        # ì´ ì›Œë“œ ë²¡í„°ëŠ” ì˜ë¯¸ ë²¡í„° ê³µê°„ì—ì„œ ë‹¨ì–´ì˜ ì¶”ìƒì  í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ë‹¨ì–´ì˜ ì¶”ìƒì ì¸ íŠ¹ì§•ë“¤ì„ ë” ì¡ì•„ë‚¼ ìˆ˜ ìˆì§€ë§Œ\n",
    "# ê·¸ë§Œí¼ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆì¢‹ì€ ê²°ê³¼ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤!   \n",
    "embedding_size = 256 # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜ë¥¼ ë§í•˜ë©° ë‹¨ì–´ê°€ ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” í¬ê¸°ì…ë‹ˆë‹¤.\n",
    "hidden_size = 1024 # ëª¨ë¸ì— ì–¼ë§ˆë‚˜ ë§ì€ ì¼ê¾¼ì„ ë‘˜ ê²ƒì¸ê°€? ì •ë„ë¡œ ì´í•´í•˜ë©´ ì¢‹ë‹¤.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_wordsì— +1ì¸ ì´ìœ ëŠ” ë¬¸ì¥ì— ì—†ëŠ” padê°€ ì‚¬ìš©ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d44eaf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 346, 7001), dtype=float32, numpy=\n",
       "array([[[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [ 7.6205324e-05, -1.6223812e-04,  1.8736023e-04, ...,\n",
       "         -6.8954495e-04, -6.1355042e-04, -2.3813831e-04],\n",
       "        [-4.2364380e-04, -2.8572470e-04,  6.2419637e-04, ...,\n",
       "         -7.9256238e-04, -5.9019483e-04, -5.7769561e-04],\n",
       "        ...,\n",
       "        [ 9.2899464e-03, -5.3402223e-04, -3.0620648e-03, ...,\n",
       "          4.6414090e-04, -4.4418084e-03,  2.8332844e-03],\n",
       "        [ 9.2899464e-03, -5.3402269e-04, -3.0620652e-03, ...,\n",
       "          4.6414055e-04, -4.4418089e-03,  2.8332840e-03],\n",
       "        [ 9.2899464e-03, -5.3402362e-04, -3.0620648e-03, ...,\n",
       "          4.6414090e-04, -4.4418089e-03,  2.8332849e-03]],\n",
       "\n",
       "       [[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [ 9.5977302e-05, -6.5519664e-05,  2.8084707e-04, ...,\n",
       "         -6.7421689e-04, -5.6543160e-04, -8.3791820e-05],\n",
       "        [ 1.7519278e-04,  3.8267576e-04,  3.2259099e-04, ...,\n",
       "         -5.5663794e-04, -9.4081572e-04,  5.7677040e-05],\n",
       "        ...,\n",
       "        [ 9.2899473e-03, -5.3402130e-04, -3.0620666e-03, ...,\n",
       "          4.6414370e-04, -4.4418094e-03,  2.8332849e-03],\n",
       "        [ 9.2899473e-03, -5.3402130e-04, -3.0620669e-03, ...,\n",
       "          4.6414323e-04, -4.4418094e-03,  2.8332851e-03],\n",
       "        [ 9.2899464e-03, -5.3402199e-04, -3.0620666e-03, ...,\n",
       "          4.6414300e-04, -4.4418094e-03,  2.8332844e-03]],\n",
       "\n",
       "       [[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [ 2.3816981e-05,  1.1990281e-04,  2.1878714e-04, ...,\n",
       "         -7.1692851e-04, -7.3212164e-04, -4.7688413e-04],\n",
       "        [-2.1913071e-04,  2.4091431e-04,  4.0692306e-04, ...,\n",
       "         -7.3443085e-04, -1.4143845e-03, -8.5012469e-04],\n",
       "        ...,\n",
       "        [ 9.2899473e-03, -5.3402339e-04, -3.0620655e-03, ...,\n",
       "          4.6414300e-04, -4.4418094e-03,  2.8332863e-03],\n",
       "        [ 9.2899464e-03, -5.3402316e-04, -3.0620657e-03, ...,\n",
       "          4.6414300e-04, -4.4418089e-03,  2.8332863e-03],\n",
       "        [ 9.2899473e-03, -5.3402409e-04, -3.0620648e-03, ...,\n",
       "          4.6414300e-04, -4.4418089e-03,  2.8332858e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [-1.0024190e-04, -4.3201409e-04, -1.0568598e-04, ...,\n",
       "         -6.6690391e-04, -4.6892982e-04, -8.2218059e-05],\n",
       "        [-5.5955682e-04, -3.9292721e-04, -1.5613486e-04, ...,\n",
       "         -1.0456697e-03, -4.5343424e-04, -4.2752374e-04],\n",
       "        ...,\n",
       "        [ 9.2899483e-03, -5.3402176e-04, -3.0620643e-03, ...,\n",
       "          4.6414183e-04, -4.4418080e-03,  2.8332844e-03],\n",
       "        [ 9.2899483e-03, -5.3402223e-04, -3.0620645e-03, ...,\n",
       "          4.6414230e-04, -4.4418080e-03,  2.8332844e-03],\n",
       "        [ 9.2899473e-03, -5.3402293e-04, -3.0620641e-03, ...,\n",
       "          4.6414183e-04, -4.4418084e-03,  2.8332842e-03]],\n",
       "\n",
       "       [[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [ 2.3735245e-04, -2.1026444e-04,  4.1843005e-04, ...,\n",
       "         -5.6150567e-04, -4.3185620e-04,  2.2638601e-04],\n",
       "        [ 3.0126696e-04, -5.5947230e-04,  7.9773262e-04, ...,\n",
       "         -3.4477207e-04, -6.3422241e-04,  9.1750015e-05],\n",
       "        ...,\n",
       "        [ 9.2899464e-03, -5.3402316e-04, -3.0620636e-03, ...,\n",
       "          4.6414090e-04, -4.4418098e-03,  2.8332849e-03],\n",
       "        [ 9.2899464e-03, -5.3402362e-04, -3.0620638e-03, ...,\n",
       "          4.6414032e-04, -4.4418103e-03,  2.8332849e-03],\n",
       "        [ 9.2899464e-03, -5.3402339e-04, -3.0620638e-03, ...,\n",
       "          4.6414067e-04, -4.4418098e-03,  2.8332849e-03]],\n",
       "\n",
       "       [[ 4.8530354e-05, -1.3453486e-05, -2.7568327e-05, ...,\n",
       "         -3.8169822e-04, -2.7582669e-04, -1.2392255e-04],\n",
       "        [-2.0977369e-04, -2.9646332e-04,  7.0012095e-05, ...,\n",
       "         -4.0590638e-04, -5.9146364e-04, -5.8328686e-04],\n",
       "        [-6.0525909e-04, -4.5794403e-04,  2.0567546e-04, ...,\n",
       "         -4.7321533e-04, -7.0707902e-04, -1.0281689e-03],\n",
       "        ...,\n",
       "        [ 9.2899473e-03, -5.3402316e-04, -3.0620655e-03, ...,\n",
       "          4.6414125e-04, -4.4418103e-03,  2.8332849e-03],\n",
       "        [ 9.2899483e-03, -5.3402362e-04, -3.0620657e-03, ...,\n",
       "          4.6414102e-04, -4.4418103e-03,  2.8332856e-03],\n",
       "        [ 9.2899483e-03, -5.3402316e-04, -3.0620659e-03, ...,\n",
       "          4.6414102e-04, -4.4418103e-03,  2.8332861e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ë´…ë‹ˆë‹¤\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4d6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  1792256   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  5246976   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               multiple                  8392704   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  7176025   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ecc60",
   "metadata": {},
   "source": [
    "# Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac033ba2",
   "metadata": {},
   "source": [
    "# Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e301211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizerì™€ lossë“±ì€ ì°¨ì°¨ ë°°ì›ë‹ˆë‹¤\n",
    "# í˜¹ì‹œ ë¯¸ë¦¬ ì•Œê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# ì–‘ì´ ìƒë‹¹íˆ ë§ì€ í¸ì´ë‹ˆ ì§€ê¸ˆ ë³´ëŠ” ê²ƒì€ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n",
    "\n",
    "# Adam ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ëŠ” optimzierì´ë©° ì–´ë–¤ optimzierë¥¼ ì¨ì•¼í• ì§€ ëª¨ë¥¸ë‹¤ë©´ Adamì„ ì“°ëŠ” ê²ƒë„ ë°©ë²•ì´ë‹¤.\n",
    "# ìš°ë¦¬ê°€ í•™ìŠµì„ í•  ë•Œ ìµœëŒ€í•œ í‹€ë¦¬ì§€ ì•ŠëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ í•´ì•¼í•œë‹¤.\n",
    "# ì—¬ê¸°ì„œ ì–¼ë§ˆë‚˜ í‹€ë¦¬ëŠ”ì§€(loss)ë¥¼ ì•Œê²Œí•˜ëŠ” í•¨ìˆ˜ê°€ ì†ì‹¤í•¨ìˆ˜ ì´ë‹¤.\n",
    "# ì´ ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ê²ƒì„ í•™ìŠµì˜ ëª©í‘œë¡œ í•˜ë©° ì—¬ê¸°ì„œ ìµœì†Œê°’ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì„ optimization ì´ë¼í•˜ê³ \n",
    "# ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ optimizer(ìµœì í™”)ë¼ê³  í•œë‹¤.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # Adamì€ í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €ì´ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì°¨ì°¨ ë°°ìš´ë‹¤.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # í›ˆë ¨ ë°ì´í„°ì˜ ë¼ë²¨ì´ ì •ìˆ˜ì˜ í˜•íƒœë¡œ ì œê³µë  ë•Œ ì‚¬ìš©í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜ì´ë‹¤.\n",
    "    from_logits=True, # ê¸°ë³¸ê°’ì€ Falseì´ë‹¤. ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ ì¶œë ¥ ê°’ì´ ì •ê·œí™”ë˜ì§€ ì•Šì•˜ìŒì„ ì†ì‹¤ í•¨ìˆ˜ì— ì•Œë ¤ì¤€ë‹¤. ì¦‰ softmaxí•¨ìˆ˜ê°€ ì ìš©ë˜ì§€ ì•Šì•˜ë‹¤ëŠ”ê±¸ ì˜ë¯¸í•œë‹¤. \n",
    "    reduction='none'  # ê¸°ë³¸ê°’ì€ SUMì´ë‹¤. ê°ì ë‚˜ì˜¤ëŠ” ê°’ì˜ ë°˜í™˜ ì›í•  ë•Œ Noneì„ ì‚¬ìš©í•œë‹¤.\n",
    ")\n",
    "# ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤í‚¤ ìœ„í•œ í•™ìŠµê³¼ì •ì„ ì„¤ì •í•˜ëŠ” ë‹¨ê³„ì´ë‹¤.\n",
    "model.compile(loss=loss, optimizer=optimizer) # ì†ì‹¤í•¨ìˆ˜ì™€ í›ˆë ¨ê³¼ì •ì„ ì„¤ì •í–ˆë‹¤.\n",
    "model.fit(dataset, epochs=30) # ë§Œë“¤ì–´ë‘” ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. 30ë²ˆ í•™ìŠµì„ ë°˜ë³µí•˜ê² ë‹¤ëŠ” ì˜ë¯¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "# generate_text í•¨ìˆ˜ì— lyricist ë¼ ì •ì˜í•œ ëª¨ë¸ì„ ì´ìš©í•´ì„œ ilove ë¡œ ì‹œì‘ë˜ëŠ” ë¬¸ì¥ì„ ìƒì„±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
